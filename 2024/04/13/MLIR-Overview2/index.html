<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第1章 绪论MLIR是多层IR的简称，为什么需要引入MLIR？要回答这个问题需要先回顾一下当下编译器现状。我们知道LLVM最为最流行的编译基础设施，被广泛地用于各种编译器中，其中最主要的原因是LLVM框架提供了大量的基于LLVM IR的优化，同时可以将LLVM IR生成众多后端的机器码。LLVM提供的各种功能几乎都是围绕LLVM IR进行，对编译器的开发者来说非常方便，例如要实现一款新语言的编译，">
<meta property="og:type" content="article">
<meta property="og:title" content="第1章：再谈MLIR">
<meta property="og:url" content="http://example.com/2024/04/13/MLIR-Overview2/index.html">
<meta property="og:site_name" content="Inside Compiler">
<meta property="og:description" content="第1章 绪论MLIR是多层IR的简称，为什么需要引入MLIR？要回答这个问题需要先回顾一下当下编译器现状。我们知道LLVM最为最流行的编译基础设施，被广泛地用于各种编译器中，其中最主要的原因是LLVM框架提供了大量的基于LLVM IR的优化，同时可以将LLVM IR生成众多后端的机器码。LLVM提供的各种功能几乎都是围绕LLVM IR进行，对编译器的开发者来说非常方便，例如要实现一款新语言的编译，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/04/13/MLIR-Overview2/17130062044277.jpg">
<meta property="og:image" content="http://example.com/2024/04/13/MLIR-Overview2/17130062461393.jpg">
<meta property="og:image" content="http://example.com/2024/04/13/MLIR-Overview2/17130063949094.jpg">
<meta property="article:published_time" content="2024-04-13T10:56:47.000Z">
<meta property="article:modified_time" content="2025-06-15T05:58:00.514Z">
<meta property="article:author" content="inside compiler">
<meta property="article:tag" content="MLIR">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/13/MLIR-Overview2/17130062044277.jpg">


<link rel="canonical" href="http://example.com/2024/04/13/MLIR-Overview2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/04/13/MLIR-Overview2/","path":"2024/04/13/MLIR-Overview2/","title":"第1章：再谈MLIR"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>第1章：再谈MLIR | Inside Compiler</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Inside Compiler</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">35</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC1%E7%AB%A0-%E7%BB%AA%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">第1章 绪论</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">inside compiler</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/13/MLIR-Overview2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="inside compiler">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Inside Compiler">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="第1章：再谈MLIR | Inside Compiler">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第1章：再谈MLIR
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-13 18:56:47" itemprop="dateCreated datePublished" datetime="2024-04-13T18:56:47+08:00">2024-04-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-15 13:58:00" itemprop="dateModified" datetime="2025-06-15T13:58:00+08:00">2025-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MLIR/" itemprop="url" rel="index"><span itemprop="name">MLIR</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="第1章-绪论"><a href="#第1章-绪论" class="headerlink" title="第1章 绪论"></a>第1章 绪论</h1><p>MLIR是多层IR的简称，为什么需要引入MLIR？要回答这个问题需要先回顾一下当下编译器现状。我们知道LLVM最为最流行的编译基础设施，被广泛地用于各种编译器中，其中最主要的原因是LLVM框架提供了大量的基于LLVM IR的优化，同时可以将LLVM IR生成众多后端的机器码。LLVM提供的各种功能几乎都是围绕LLVM IR进行，对编译器的开发者来说非常方便，例如要实现一款新语言的编译，只需要将新语言编译成LLVM IR就可以复用LLVM的中端优化和后端代码生成能力，从而高效实现一款编译器。<br>然而随着时间的推移，我们可以发现两个问题：一方面，越来越多的语言接入LLVM IR之前都需要实现自己的前端IR，用于处理语言特殊的优化，以及方便将语言降级到LLVM IR。</p>
<p>例如现在很多高级语言都会使用LLVM作为其中后段。如下所示：</p>
<img src="/2024/04/13/MLIR-Overview2/17130062044277.jpg" class="">


<p>每个语言都会有自己的AST，除了AST以外这些语言还得有自己的IR来做language- specific optimization，但是他们的IR最后往往都会接到同样的后端，比如说LLVM IR上来做代码生成，来在不同的硬件上运行。这些语言专属的IR被叫做Mid-Level IR，而且不通语言自己的IR的优化会有重复的部分，但很难互相复用代码，重复造了很多轮子。</p>
<p>另一方面，越来越多的新硬件出现，它们通常用于专用领域，这些领域通常引入了DSL（Domain Specific Language，领域编程语言），而针对DSL的编译优化除了传统的编译优化知识外，通用还需要相关的领域知识，而这在LLVM IR通常很难表达和优化。例如TensorFlow系统其编译过程非常复杂，如下所示：</p>
<img src="/2024/04/13/MLIR-Overview2/17130062461393.jpg" class="">
<p>一个Tensorflow的Graph被执行可以有若干条途径，例如可以直接通过Tensorflow Executor来调用一些手写的op-kernel函数；或者将TensorFlow Graph转化到自己的XLA HLO，由XLA HLO再转化到LLVM IR上调用CPU、GPU或者转化到TPU IR生成TPU代码执行；对于特定的后端硬件，可以转化到TensorRT、或者像是nGraph这样的针对特殊硬件优化过的编译工具来跑；或者转化到TFLite格式进一步调用NNAPI来完成模型的推理。</p>
<p>而MLIR则是希望通过引入多层IR的方式解决上面的两个问题：通过多层IR提供方便DSL接入，同时提供针对领域相关的优化。下面通过一个例子直接的看一下MLIR的基本概念。<br>假设我们有一个PyTorch的模型，代码如</p>
<figure class="highlight plaintext"><figcaption><span>Linear(nn.Module):</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self): </span><br><span class="line">super(Linear, self).__init__()</span><br><span class="line">self.linear = nn.Linear(16, 10)</span><br><span class="line"></span><br><span class="line">def forward(self, x): </span><br><span class="line">return self.linear(x)</span><br><span class="line"></span><br><span class="line">linear = Linear()</span><br><span class="line">mlir_module = torch_mlir.compile(linear, torch.ones( 1, 16), output_type=torch_mlir.OutputType.TOSA)</span><br></pre></td></tr></table></figure>

<p>代码使用Linear建立一个全联接的神经网络，这个神经网络做的事情非常简单，对于输入x计算得到y，而矩阵A和骗至b是网络模型参数，在神经网络训练时得到参数，在推理时使用参数。</p>
<img src="/2024/04/13/MLIR-Overview2/17130063949094.jpg" class="">
<p>而作为编译器开发者希望模型执行足够快，所以可以通过编译的方式生成可执行的代码，并在编译过程进行优化。向PyTorch这样的AI框架通常会将代码变成HIR和LIR，分别进行图优化和算子优化，然后再生成代码，正如图2提到的一样，除了编译和优化工作外需要框架考虑不同后端。<br>而MLIR则是期望通过设计多层IR表达不同层次的功能，让编译器都能重用这些IR，同时在MLIR中对这次不同层次的IR进行针对性的优化，从而达到最优性能。</p>
<p>例如在MLIR设计了一个接入层IR（实际上称为方言）TOSA（Tensor Operation Set Architecture），可以将上述代码转换为TOSA方言表达的代码。</p>
<figure class="highlight plaintext"><figcaption><span>@forward(%arg0: tensor<1x16xf32>) -> tensor<1x10xf32> &#123;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    %0 = &quot;tosa.const&quot;() &#123;value = dense&lt;&quot;0xC44B...&quot;&gt; : tensor&lt;1x16xf32&gt;&#125; : () -&gt; tensor&lt;1x16xf32&gt;</span><br><span class="line">    %1 = &quot;tosa.const&quot;() &#123;value = dense&lt;&quot;0xA270...&quot;&gt; : tensor&lt;1x10xf32&gt;&#125; : () -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    %2 = &quot;tosa.reshape&quot;(%arg0) &#123;new_shape = [1, 1, 16]&#125; : (tensor&lt;1x16xf32&gt;) -&gt; tensor&lt;1x16xf32&gt;</span><br><span class="line">    %3 = &quot;tosa.matmul&quot;(%2, %0) : (tensor&lt;1x1x16xf32&gt;, tensor&lt;1x16x10xf32&gt;) -&gt; tensor&lt;1x1x10xf32&gt;</span><br><span class="line">    %4 = &quot;tosa.reshape&quot;(%3) &#123;new_shape = [1, 10]&#125; : (tensor&lt;1x1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    %5 = &quot;tosa.add&quot;(%4, %1) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    return %5 : tensor</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>经过这样的处理后，则就将Python代码描述的模型转换为MLIR代码。这里先暂不对MLIR进行详细介绍，我们仅仅简单介绍如何阅读上述代码。</p>
<ol>
<li>形如“dialect.operation”的字符串表示，方言为dialenct，操作为operation，方言的目的管理Operation，而Operation表述一定功能。例如func.func表示func方言里面的func操作。上述整个代码表示定义一个func方言的func操作。</li>
<li>形如“%arg0: tensor&lt;1x16xf32&gt;”，其中%arg0表示变量名，tensor&lt;1x16xf32&gt;表示类型。这里%arg0时参数，其类型为tensor类型，并且tesnor是二维的，第一维的长度为1，地二维的长度为16，tensor的数据元素类型为float32（简写f32）。</li>
<li>形如“%0 &#x3D; &#x3D; “tosa.const”() {value &#x3D; dense&lt;”0xC44B…”&gt; : tensor&lt;1x16xf32&gt;} : () -&gt; tensor&lt;1x16xf32&gt;”中的%0表示临时定义的变量；它使用tosa方言的const操作生成，其中const操作可以接受属性参数，其属性为value，而value是dense类型，value的类型为tensor&lt;1x16xf32&gt;，%0的类型也是tensor&lt;1x16xf32&gt;。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//定义函数forward，接受参数arg0，参数类型为tensor&lt;1x16xf32&gt;，函数的返回类型为tensor&lt;1x10xf32&gt;</span><br><span class="line">func.func @forward(%arg0: tensor&lt;1x16xf32&gt;) -&gt; tensor&lt;1x10xf32&gt; &#123;</span><br><span class="line">//定义常量，类型为tensor&lt;1x16xf32&gt;。常量是通过tosa.const操作创建，tosa.const操作接受属性value，其中value类型为tensor&lt;1x16xf32&gt;</span><br><span class="line">    %0 = &quot;tosa.const&quot;() &#123;value = dense&lt;&quot;0xC44B...&quot;&gt; : tensor&lt;1x16xf32&gt;&#125; : () -&gt; tensor&lt;1x16xf32&gt;</span><br><span class="line">    //定义常量，类型为tensor&lt;1x10xf32&gt;。常量是通过tosa.const操作创建，tosa.const操作接受属性value，其中value类型为tensor&lt;1x10xf32&gt;</span><br><span class="line">    %1 = &quot;tosa.const&quot;() &#123;value = dense&lt;&quot;0xA270...&quot;&gt; : tensor&lt;1x10xf32&gt;&#125; : () -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    //对arg0进行类型进行变换，从类型tensor&lt;1x16xf32&gt;变成tensor&lt;1x1x16xf32&gt;</span><br><span class="line">    %2 = &quot;tosa.reshape&quot;(%arg0) &#123;new_shape = [1, 1, 16]&#125; : (tensor&lt;1x16xf32&gt;) -&gt; tensor&lt;1x1x16xf32&gt;</span><br><span class="line">    //对%2和%0进行类matmul计算，输入类型为tensor&lt;1x1x16xf32&gt;, tensor&lt;1x16x10xf32&gt;，输出类型为tensor&lt;1x1x10xf32&gt;</span><br><span class="line">    %3 = &quot;tosa.matmul&quot;(%2, %0) : (tensor&lt;1x1x16xf32&gt;, tensor&lt;1x16x10xf32&gt;) -&gt; tensor&lt;1x1x10xf32&gt;</span><br><span class="line">    //对%3进行类型进行变换，从类型tensor&lt;1x1x10xf32&gt;变成tensor&lt;1x10xf32&gt;</span><br><span class="line">    %4 = &quot;tosa.reshape&quot;(%3) &#123;new_shape = [1, 10]&#125; : (tensor&lt;1x1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    //对%4和%1进行张量加法，输入和输出类型都是tensor&lt;1x10xf32&gt;</span><br><span class="line">    %5 = &quot;tosa.add&quot;(%4, %1) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    //返回%5，类型为tensor&lt;1x10xf32&gt;</span><br><span class="line">    return %5 : tensor</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>从上面的代码注释可以看出，它是Python代码的另外一种实现。也可以说通过工具将Python代码实现成为以tosa方言中的操作。</p>
<p>虽然通过TOSA方言可以将Python代码表示出来，但是TSOA中的操作非常高级，需要进一步降级，从而描述如何实现这些操作。例如matmul执行的是矩阵乘，而矩阵乘法需要通过循环实现。</p>
<p>在MLIR社区中提供了linalg方言，它有一些命名操作（如matmul等）和通用操作（如generic），这个方言是承上启下的，接受上层代码的降级，同时提供一些优化功能，并降级到更为底层的方言。例如上面的代码可以进一步降级为使用linlag方言描述的代码，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#map0 = affine_map&lt;(d0, d1, d2) -&gt; (d0, d2)&gt;</span><br><span class="line">#map1 = affine_map&lt;(d0, d1, d2) -&gt; (d2, d1)&gt;</span><br><span class="line">#map2 = affine_map&lt;(d0, d1, d2) -&gt; (d0, d1)&gt; </span><br><span class="line">func.func @forward(%arg0: tensor&lt;1x16xf32&gt;) -&gt; tensor&lt;1x10xf32&gt; &#123;</span><br><span class="line">  %cst = arith.constant dense&lt;&quot;0xA270...&quot;&gt; : tensor&lt;1x10xf32&gt;</span><br><span class="line">    %cst_0 = arith.constant dense&lt;&quot;0xC44B...&quot;&gt; : tensor&lt;16x10xf32&gt;</span><br><span class="line">    %0 = linalg.generic &#123;indexing_maps = [#map0, #map1, #map2], iterator_types = [&quot;parallel&quot;, &quot;parallel&quot;, &quot;reduction&quot;]&#125; ins(%arg0, %cst_0 : tensor&lt;1x16xf32&gt;, tensor&lt;16x10xf32&gt;) outs(%cst : tensor&lt;1x10xf32&gt;)</span><br><span class="line">    &#123;</span><br><span class="line">        ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):</span><br><span class="line">            %1 = arith.mulf %arg1, %arg2 : f32</span><br><span class="line">            %2 = arith.addf %arg3, %1 : f32</span><br><span class="line">            linalg.yield %2 : f32</span><br><span class="line">    &#125; -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    return %0 : tensor&lt;1x10xf32&gt;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>在上面的代码中有两类特殊的操作，分别是affine_map和linalg.generic。其中affine_map定义的仿射变换的定义域和值域，例如#map0 &#x3D; affine_map&lt;(d0, d1, d2) -&gt; (d0, d2)&gt;表示定义一个仿射变换，输入的定义域可以通过三个维度(d0, d1, d2)遍历得到，而输出的值域通过二个维度(d0, d2)遍历得到。<br>而linalg.eneric则是提供复杂的操作，它的输入有仿射变换规则、迭代方式，输入和输出参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//定义仿射变换</span><br><span class="line">#map0 = affine_map&lt;(d0, d1, d2) -&gt; (d0, d2)&gt;</span><br><span class="line">#map1 = affine_map&lt;(d0, d1, d2) -&gt; (d2, d1)&gt;</span><br><span class="line">#map2 = affine_map&lt;(d0, d1, d2) -&gt; (d0, d1)&gt; </span><br><span class="line">func.func @forward(%arg0: tensor&lt;1x16xf32&gt;) -&gt; tensor&lt;1x10xf32&gt; &#123;</span><br><span class="line">    %cst = arith.constant dense&lt;&quot;0xA270...&quot;&gt; : tensor&lt;1x10xf32&gt;</span><br><span class="line">    %cst_0 = arith.constant dense&lt;&quot;0xC44B...&quot;&gt; : tensor&lt;16x10xf32&gt;</span><br><span class="line">    //定义linalg的通用操作，这个操作接受属性indexing_map、iterator_types，描述的针对输入参数%args0和%cst_0进行迭代，生成输出%cst</span><br><span class="line">    %0 = linalg.generic &#123;indexing_maps = [#map0, #map1, #map2], iterator_types = [&quot;parallel&quot;, &quot;parallel&quot;, &quot;reduction&quot;]&#125; ins(%arg0, %cst_0 : tensor&lt;1x16xf32&gt;, tensor&lt;16x10xf32&gt;) outs(%cst : tensor&lt;1x10xf32&gt;)</span><br><span class="line">    &#123;</span><br><span class="line">        //这是一个基本块，和一般的SSA不同，这里基本块有参数arg1、arg2和args3.</span><br><span class="line">        ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):</span><br><span class="line">            // arg1和arg2相乘，在arg3相加得到输出。</span><br><span class="line">            %1 = arith.mulf %arg1, %arg2 : f32</span><br><span class="line">            %2 = arith.addf %arg3, %1 : f32</span><br><span class="line">            linalg.yield %2 : f32</span><br><span class="line">    &#125; -&gt; tensor&lt;1x10xf32&gt;</span><br><span class="line">    return %0 : tensor&lt;1x10xf32&gt;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>注意：这里仅仅是演示其中一种降级方法，在这个方法中可以看到乘法和加法都放在基本块中。当然还可以先乘后加。如何降级是非常复杂的，在后续文章会详细介绍。</p>
<p>同理linalg中的操作非常复杂，generic仅仅描述了它的功能，具体的实现仍然不确定，所以进一步使用仿射进行描述其真实的实现，结果如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">memref.global &quot;private&quot; constant @__constant_16x10xf32 : memref&lt;16x10xf32&gt; = dense&lt;&quot;0xC44B...&quot;&gt;</span><br><span class="line">memref.global &quot;private&quot; constant @__constant_1x10xf32 : memref&lt;1x10xf32&gt; = dense&lt;&quot;0xA270...&quot;&gt;</span><br><span class="line">func.func @forward(%arg0: memref&lt;1x16xf32&gt;, %arg1: memref&lt;1x10xf32&gt;) &#123;</span><br><span class="line">    %0 = memref.get_global @__constant_1x10xf32 : memref&lt;1x10xf32&gt;</span><br><span class="line">    %1 = memref.get_global @__constant_16x10xf32 : memref&lt;16x10xf32&gt;</span><br><span class="line">    memref.copy %0, %arg1 : memref&lt;1x10xf32&gt; to memref&lt;1x10xf32&gt;</span><br><span class="line">    affine.for %arg2 = 0 to 10 &#123;</span><br><span class="line">        affine.for %arg3 = 0 to 16 &#123;</span><br><span class="line">            %2 = affine.load %arg0[0, %arg3] : memref&lt;1x16xf32&gt;</span><br><span class="line">            %3 = affine.load %1[%arg3, %arg2] : memref&lt;16x10xf32&gt;</span><br><span class="line">            %4 = affine.load %arg1[0, %arg2] : memref&lt;1x10xf32&gt;</span><br><span class="line">            %5 = arith.mulf %2, %3 : f32</span><br><span class="line">            %6 = arith.addf %4, %5 : f32</span><br><span class="line">            affine.store %6, %arg1[0, %arg2] : memref&lt;1x10xf32&gt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个代码片段中可以看出其实现已经非常接近我们传统的代码，例如memref方言描述的数据的内存布局，affine.for表示的是一个循环，affine.load和affine.store描述的是如何从memref加载、写数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//定义个全局常量，并提供了初始化的数据</span><br><span class="line">memref.global &quot;private&quot; constant @__constant_16x10xf32 : memref&lt;16x10xf32&gt; = dense&lt;&quot;0xC44B...&quot;&gt;</span><br><span class="line">memref.global &quot;private&quot; constant @__constant_1x10xf32 : memref&lt;1x10xf32&gt; = dense&lt;&quot;0xA270...&quot;&gt;</span><br><span class="line">func.func @forward(%arg0: memref&lt;1x16xf32&gt;, %arg1: memref&lt;1x10xf32&gt;) &#123;</span><br><span class="line">    %0 = memref.get_global @__constant_1x10xf32 : memref&lt;1x10xf32&gt;</span><br><span class="line">    %1 = memref.get_global @__constant_16x10xf32 : memref&lt;16x10xf32&gt;</span><br><span class="line">    // 为arg1赋初值，使用copy操作进行</span><br><span class="line">    memref.copy %0, %arg1 : memref&lt;1x10xf32&gt; to memref&lt;1x10xf32&gt;</span><br><span class="line">    //定义外层循环，循环空间从0到10，步长默认为1</span><br><span class="line">    affine.for %arg2 = 0 to 10 &#123;</span><br><span class="line">         //定义内层循环，循环空间从0到16，步长默认为1</span><br><span class="line">        affine.for %arg3 = 0 to 16 &#123;</span><br><span class="line">            %2 = affine.load %arg0[0, %arg3] : memref&lt;1x16xf32&gt;</span><br><span class="line">            %3 = affine.load %1[%arg3, %arg2] : memref&lt;16x10xf32&gt;</span><br><span class="line">            %4 = affine.load %arg1[0, %arg2] : memref&lt;1x10xf32&gt;</span><br><span class="line">            %5 = arith.mulf %2, %3 : f32</span><br><span class="line">            %6 = arith.addf %4, %5 : f32</span><br><span class="line">            affine.store %6, %arg1[0, %arg2] : memref&lt;1x10xf32&gt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用Affine方言描述的代码就非常容易转换到LLVM IR，得到的LLVM IR如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">    ... ...</span><br><span class="line">    ^bb1(%20: i64): // 2 preds: ^bb0, ^bb4</span><br><span class="line">    %21 = llvm.icmp &quot;slt&quot; %20, %5 : i64</span><br><span class="line">    llvm.cond_br %21, ^bb2(%4 : i64),</span><br><span class="line">    ^bb5 ^bb2(%22: i64): // 2 preds: ^bb1, ^bb3</span><br><span class="line">    %23 = llvm.icmp &quot;slt&quot; %22, %7 : i64</span><br><span class="line">    llvm.cond_br %23, ^bb3, ^bb4</span><br><span class="line">    ^bb3: // pred: ^bb2</span><br><span class="line">    ... ...</span><br><span class="line">    %46 = llvm.intr.masked.load %45, %36, %0 &#123;alignment = 4 : i32&#125; : (!llvm.ptr&lt;vector&lt;2xf32&gt;&gt;, vector&lt;2xi1&gt;, vector&lt;2xf32&gt;) -&gt; vector&lt;2xf32&gt;</span><br><span class="line">    %47 = llvm.fmul %30, %41 : vector&lt;2xf32&gt;</span><br><span class="line">    %48 = llvm.fadd %46, %47 : vector&lt;2xf32&gt; llvm.intr.masked.store %48, %45, %36 &#123;alignment = 4 : i32&#125; : vector&lt;2xf32&gt;, vector&lt;2xi1&gt; into !llvm.ptr&lt;vector&lt;2xf32&gt;&gt;</span><br><span class="line">    %49 = llvm.add %22, %8 : i64 llvm.br ^bb2(%49 : i64)</span><br><span class="line">    ^bb4: // pred: ^bb2</span><br><span class="line">    %50 = llvm.add %20, %6 : i64</span><br><span class="line">    llvm.br ^bb1(%50 : i64)</span><br><span class="line">    ^bb5: // pred: ^bb1</span><br><span class="line">    llvm.return</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>然后再利用LLVM可以将LLVM IR进行优化以及针对目标架构完成代码生成。<br>具体转换过程可参考：<br><a target="_blank" rel="noopener" href="https://file.elecfans.com/web2/M00/7E/0E/poYBAGOC6bKAZAQyADt7O8jLZCE607.pdf">https://file.elecfans.com/web2/M00/7E/0E/poYBAGOC6bKAZAQyADt7O8jLZCE607.pdf</a></p>
<p>通过这个例子，我们可以进一步得到如下信息：</p>
<ol>
<li>MLIR通过多方言的形式，逐步将抽象、高层的代码降级到底层代码。</li>
<li>降级过程中使用了一个非常便于优化的方言，例如Affine是多面体编译的抽象，非常方便进行循环相关的优化</li>
<li>在降级过程并不唯一，开发者可以根据自己的代码意图选择合适的降级路线。从而实现代码性能最优。</li>
</ol>
<span id="more"></span>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/MLIR/" rel="tag"># MLIR</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/16/ADT-Algorithm/" rel="prev" title="ADT-算法">
                  <i class="fa fa-angle-left"></i> ADT-算法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/19/MLIR-Overview3/" rel="next" title="第2章：MLIR-设计、实现与架构">
                  第2章：MLIR-设计、实现与架构 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">inside compiler</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"inside-compiler","repo":"inside-compiler.github.io","client_id":"3b662fd9edd646229c79","client_secret":"312a2514c4029a47341087757f0c051786144c77","admin_user":"fiking","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"labels":["gitalk"],"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"214a6dc9d62f80a9ae0bee2e4777b354"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
